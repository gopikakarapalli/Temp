1)  Understanding Activation Functions in Neural Networks
	Link: https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0
2) What are the advantages of ReLU over sigmoid function in deep neural networks?
	Link: https://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-networks
3) Types of Optimization Algorithms used in Neural Networks and Ways to Optimize Gradient Descent
	link: https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f
4) Up-sampling with Transposed Convolution:
	link: https://medium.com/activating-robotic-minds/up-sampling-with-transposed-convolution-9ae4f2df52d0
	
	
5) Deep Learning : Autoencoders Fundamentals and types 
	LINK: https://codeburst.io/deep-learning-types-and-autoencoders-a40ee6754663
	Link:  https://iq.opengenus.org/types-of-autoencoder/
6) unet:https:
         linl: //towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47
	 link2:https://towardsdatascience.com/u-net-b229b32b4a71
	 code Link:https://github.com/zhixuhao/unet
	 data set: https://www.kaggle.com/c/tgs-salt-identification-challenge/data

